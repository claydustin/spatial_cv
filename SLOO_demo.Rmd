---
title: Spatial Leave One Out (SLOO) (Le Rest et al 2014)
author: "Dan McGlinn"
date: "Tuesday, March 31, 2015"
output: html_document
---

This document is designed to demonstrate how the Spatial Leave One Out (SLOO)
approach of Le Rest et al (2014) works. There original R code can be found
online https://sites.google.com/site/lerestk/publications and in this repo here
./scripts/lerest_2014. 

This document modifies their code for simplicity generality and ease of learning:

## Spatially define training data

Below the Le Rest function splitting has been modified to allow for latitude and
longitude coordinates.Splitting is exactly what the function does. It actually
gets the rows associated with the training dataset so I renamed it.
```{r}
#' Get the row indices for the training data sets
#'
#' For each sample in the dataset a set of indices for the training
#' data to compare to that sample is provided based on a threshold
#' distance. If the threshold distance is zero then all other samples
#' are included in the training set.
#'
#' @param coords 2 dimensional matrix of spatial coordinates
#' @param dist_thres the threshold distance, if longlat = TRUE
#' this is in units of kilometers
#' @longlat boolean, if TRUE great circle distances are computed
#' rather than euclidean distances
#' @export
#' @examples
#' coords = expand.grid(1:4, 1:4)
#' training_rows = get_training_row(coords, 3)
#' coords[training_rows[[i]]
get_training_rows = function(coords, dist_thres, longlat=FALSE) {
    ## Computing the distance matrix from the data:
    if (longlat) {
        require(sp)
        dist_matrix = spDists(coords, longlat=TRUE)
    }
    else
        dist_matrix = as.matrix(dist(coords))
    ## Initializing the row indices of the dataset to be used in training
    training_rows = list()
    ## Creating the sets of training indices
    for (i in 1:nrow(dist_matrix)) {
        # Keeping only the observations far enough of the i-st observation by
        # using the threshold distance
        num_cell = which(dist_matrix[i, ] > dist_thres)
        training_rows[[i]] = num_cell
    }
    return(training_rows)
}
```
One quick thing to point out about is that the strange sytanx that you see
outside of the R function is markup for the R package Roxygen which is used to
automatically generate R helpfiles. So in essesnse all the marked up test that
is commented out is the helpfile for the function.

Ok so what does the function `get_training_rows` actually do? Let’s see
```{r}
# create very simple sample coordinates
coords = cbind(1:10, rep(1,10))
# now see what happens when you change dist_thres
get_training_rows(coords, dist_thres = 1)
get_training_rows(coords, dist_thres = 2)
get_training_rows(coords, dist_thres = 3)
get_training_rows(coords, dist_thres = 4)

nrow(coords) == length(get_training_rows(coords, dist_thres = 3))
nrow(coords) == length(get_training_rows(coords, dist_thres = 4))

# return the coordinates of the training dataset when the dist_thres is 3
# and the test point is the 5th sample
coords[get_training_rows(coords, dist_thres=3)[[5]], ]
# so the 1st, 9th, and 10th samples will form the training datset in this
# case all the other samples are within the distance threshold.
```

## Spatial leave one out simplified 

In Le Rest's function `sloo` this is quite a bit going on. Here I've ripped
everything out but the most important chunks. I've also renamed a few
variables that were confusingly named. 

The most confusing part to you will likely still be what's going on with
the function dnorm(). To understand this you have to understand how
model likelihoods are calculated.

When moving beyond just comphrending the code you will need to go back to 
Le Rest's original sloo.R which is more general and make modifications
for there. 

Also `debug(sloo_simple)` is a good tool to use to see exactly how `sloo_simple` is working.

```{r}
sloo_simple = function(model,training){
    # Computing Spatial Leave One Out (SLOO) logLikelihood from a model 
    # Arguments:
    # - model: an object of class 'lm' or 'glm' giving the model considered.
    # - training: the 'list' return by the 'get_training_rows' function
    # Creating the response variable name
    y <- as.character(x=formula(x=model)[2])
    # Initializing the 'logLik' object
	logLik <- vector(mode="numeric",length=nrow(x=model$data))
	# Calculating the SLOO logLikelihoods for each observation i
	for(i in 1:nrow(x=model$data)){ 
		# Extracting the i-st training set:
	    training_data <- model$data[training[[i]],]
		# Calculating the model parameters from the i-st training set:
		m <- glm(formula=formula(model),data=training_data,family= model$family)
		# Predicting the i-st observation from the i-st training set:
		m.pred <- predict(object=m,newdata=model$data[i,],type="response")
		# Calculating the probability of the i-st observed value according
        # to the predicted one by the i-st training set:
        logLik[i] <- dnorm(x=model$data[i,y],mean=m.pred,
                           sd=sqrt(sum(residuals(m)^2)/nrow(training_data)),
                           log=T)
    }
	# Calculating the overall SLOO logLikelihood:
	Sum.logLik <- sum(logLik)
	return(Sum.logLik)
}
```

So it returns a list that has as many elements as there are coordinates. Each element of the list is a vector of
row indices which are to be used as the training dataset to test against that particular row.
We have a method for generating the training datatsets for every sample in our dataset given a distance
threshold. Let’s test if this actually makes a difference when we simulate two levels of spatial autocorrleation.
We’ll simulate zero autocorrelation and somewhat strong autocorrelation.

## Simulate landscape with given level of spatial autocorrelation

```{r}
library(RandomFields)
# set the size of one side on the grid
n = 100
spat_range = 5
# you must specify a model of spatial dependence, in this
# case I use an exponential model with a range set by spat_range
mod_spat_dep = RMexp(var=1, scale=spat_range)
spat_err = RFsimulate(mod_spat_dep, x=1:n, y=1:n, grid=T)
x1 = RFsimulate(mod_spat_dep, x=1:n, y=1:n, grid=T)
x2 = RFsimulate(mod_spat_dep, x=1:n, y=1:n, grid=T)
# visually check that the simulation worked as expected
plot(x1)
# examine the variogram
plot(RFempiricalvariogram(data=x1))
# notice that it starts leveling out around the range we set
# use the simulated variable to generate a response variable that is
# dependent upon x1 and has spatially autocorrelated error
spat_err = as.vector(spat_err)
x1 = as.vector(x1)
x2 = as.vector(x2)
beta = rnorm(1, mean=0, sd=1)
beta
y = beta * x1 + spat_err
coords = cbind(rep(x=1:n, times=n), rep(x=1:n,each=n))

dataset = data.frame(y, x1, x2, coords)

## ----------------------------------------------------
# Above we’ve generated our response variable y and two
# indepdenent predictor variables x1 and x2.

nrow(dataset)
# that is too big so let's just use 50% of the data to test out the SLOO method
N_samps = 2000
row_indices = sample(nrow(dataset), size=N_samps)

mods = list()
mods$int_mod = glm(y ~ 1, data = dataset[row_indices, ])
mods$x1_mod = update(mods$int_mod, . ~ . + x1)
mods$x2_mod = update(mods$int_mod, . ~ . + x2)
mods$x12_mod = update(mods$int_mod, . ~ . + x1 + x2)

training = get_training_rows(coords[row_indices, ], dist_thres = spat_range)

# evalutate the four models using log likelihood 
# with the SLOO and a non-spatial approach
sloo_ll = sapply(mods, function(x) sloo_simple(x, training))
ll = sapply(mods, logLik) 

# these values are log likelihoods of the models
# the larger the value (i.e., less negative) the better 
sort(sloo_ll, dec=T)
sort(ll, dec=T)

# FYI the relationship between ll and AIC is given by
# AIC = 2*k - 2*ll where k is the number of parameters in the model
2* c(2, 3, 3, 4) - 2*ll
sapply(mods, AIC)

# but the above really only is theorectially justified when the 
# samples are indepenent. In other words the penalty of 2*k is derived 
# under a strick assumption of independence. 
```