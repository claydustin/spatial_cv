---
title: "Leave one out Method"
author: "Daivd Leslie"
date: "Monday, March 24, 2015"
output: pdf_document
---
The purpose of the code is to evaluate the effectiveness of the leave one out method for determining the predictive power of each model given. In order to acomplish this, the model given will be trained with data that has been reduced by one sample. The sample that is left out will then be used as a new data point to be predicted by the given model. By looking at the differences in the predicted value by the model and the actual value recorded, a greater insight will be gained as to how well the model preforms when predicting new data. Let's first start by generating a popluation, which we will sample from.

```{r}
# Create predictor variables
set.seed(10)
pop = 9000
x1 = rnorm(pop)
x2 = rnorm(pop)
x3 = rnorm(pop)

# Create noise term
noise = rnorm(pop,0,3)

# Generate responce: additive model plus noise, intercept = 0
y = 2*x1 + x2 + 3*x3 + noise

# Organize predictors and responce in data frame
pop_data = data.frame(y, x1, x2, x3)

head(pop_data)

```

Now that we have our population, lets take a sample of our data and verify that the results generated by the model are reasonalbe estimates of the true model's coeffcients (Remebering that they should be close to the exspected values of: x1=2, x2=1, and x3=3).  

```{r}
# Create sample population
n = 375
samp = pop_data[sample(nrow(pop_data), n), ]
# Create model(s)
mod = lm(y ~ x1 + x2 + x3, data=samp)

summary(mod)

```

Looking at the model summary, it appears that calculated coeffeicents are fairly close to actual coefficents. It also appears that approximately 60% of the variability can be explained by the predictor variables. This r squared value seems reasonalbe since there is an extra error term incorporated into the model (noise). Now that we feel cofident about the sample taken, lets see how good the models generated will be at predicting new data. In order to do this, lets apply the leave one out method to the data. The two things to note for this method are:

1. How close are the predicted values to the actual vales.

2. How much the coeffcients vary between models

```{r}
# Function(s)
get_R2 = function(obs, pred, na.rm=FALSE) {
  if (na.rm) {
    true = !(is.na(obs) | is.na(pred))
    obs = obs[true]
    pred = pred[true]
  }
  SSerr = sum((obs - pred)^2)
  SStot = sum((obs - mean(obs))^2)
  R2 = 1 - SSerr / SStot
  return(R2)
}

Leave_one_out = function(data, model) {
  matrixCoef = matrix(NA, nrow=nrow(data), ncol=length(coef(model)))
  colnames(matrixCoef) = names(coef(model))
  predicted = NULL
  rSquared = NULL
  for(i in 1:nrow(data)) {
    trainSet = data[-i,]
    testSet = data[i,]
    trainMod = update(model, data=trainSet)
    matrixCoef[i, ] = coef(trainMod)
    predicted[i] = predict(trainMod, newdata = testSet)
    rSquared[i] = summary(trainMod)$r.squared
    rSquaredCalc = get_R2(residuals(trainMod) + predict(trainMod), predict(trainMod))
  }
  observed = residuals(mod) + predict(mod)
  finalMatrix = data.frame(matrixCoef, predicted, observed, rSquared, rSquaredCalc)
  names(finalMatrix)[1] = 'intercept'
  return(finalMatrix)
}

fit_mod = lm(y ~ x1 + x2 + x3, data=samp)
resultsMatrix = Leave_one_out(samp, fit_mod)

plot(observed ~ predicted, data=resultsMatrix)
abline(a=0, b=1)

head(resultsMatrix)
```
From the results, it appears that the coeffeicents are not varying too much, which would suggest that the models are not overfitting the data. However, the differences in the predicted and actual values do vary slightly with each model prediction. Despite this varation, all the models constructed have relatively the same r squared value of roughtly 60%, which would suggest that all of the models have about the same predictive power.  